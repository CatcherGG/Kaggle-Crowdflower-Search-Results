{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowdflower Search Results Relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports. (Nothing to see here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import BaseEstimator,TransformerMixin\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.ensemble import RandomForestRegressor,AdaBoostRegressor,BaggingRegressor,ExtraTreesRegressor,GradientBoostingRegressor,RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDRegressor,PassiveAggressiveRegressor,LassoLars\n",
    "from sklearn.metrics import mean_squared_error,classification_report,accuracy_score\n",
    "from sklearn.svm import SVR,SVC\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import re\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "import cPickle as pickle\n",
    "import math\n",
    "import gc\n",
    "import warnings\n",
    "from collections import Counter\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"train.csv\").fillna(\"\")\n",
    "test  = pd.read_csv(\"test.csv\").fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let see what hides behind the csvs..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'query',\n",
       " 'product_title',\n",
       " 'product_description',\n",
       " 'median_relevance',\n",
       " 'relevance_variance']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id', 'query', 'product_title', 'product_description']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process.\n",
    "Our pre-process was including 3 main steps:\n",
    "1. Cleaning HTML.\n",
    "2. Removing stop words.\n",
    "3. Stemming.\n",
    "\n",
    "In any case, We saved 3 versions of the given columns - Original version, After cleaning [Stop words & HTML], After stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def removeUnnecessaryText(df):\n",
    "    regex = re.compile(\"(.style\\d+\\s+{[\\w \\-:;\\\"\\'\\\\#\\(\\)\\\\n]*}\\s*(\\\\n)*)*(?P<text>.*)\",re.MULTILINE)\n",
    "    return df.apply(lambda x: regex.match(BeautifulSoup(x).get_text('\\n','\\t').replace('\\n',' ')).group(\"text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def removeStopwords(df):\n",
    "    cachedStopWords = stopwords.words(\"english\")\n",
    "    return df.apply(lambda x: ' '.join([word for word in token_pattern.findall(x) if word not in cachedStopWords]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "def preprocess(df):\n",
    "    #remove html and styles\n",
    "    df[\"product_description\"] = removeUnnecessaryText(df[\"product_description\"])\n",
    "    df[\"query\"] = removeUnnecessaryText(df[\"query\"])\n",
    "    df[\"product_title\"] = removeUnnecessaryText(df[\"product_title\"])\n",
    "    print \"Done cleaning HTML\"\n",
    "    # removing stopwords\n",
    "    df[\"product_description_clean\"] = removeStopwords(df[\"product_description\"])\n",
    "    df[\"product_title_clean\"] = removeStopwords(df[\"product_title\"])\n",
    "    df[\"query_clean\"] = removeStopwords(df[\"query\"])\n",
    "    print \"Done removing stopwords\"\n",
    "    # steming the words\n",
    "    df[\"product_description_stemed\"] = df[\"product_description_clean\"].apply(lambda x: ' '.join([stemmer.stem(word) for word in token_pattern.findall(x)]))\n",
    "    df[\"product_title_stemed\"] = df[\"product_title_clean\"].apply(lambda x: ' '.join([stemmer.stem(word) for word in token_pattern.findall(x)]))\n",
    "    df[\"query_stemed\"] = df[\"query_clean\"].apply(lambda x: ' '.join([stemmer.stem(word) for word in token_pattern.findall(x)]))\n",
    "    print \"Done stemming\"\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cleaning HTML\n",
      "Done removing stopwords\n",
      "Done stemming\n"
     ]
    }
   ],
   "source": [
    "train = preprocess(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10158.000000\n",
       "mean         3.309805\n",
       "std          0.980666\n",
       "min          1.000000\n",
       "25%          3.000000\n",
       "50%          4.000000\n",
       "75%          4.000000\n",
       "max          4.000000\n",
       "Name: median_relevance, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['median_relevance'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we can see here is that most of the 'median revelance' are 4. [We'll use this information later on]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Features:\n",
    "1. BM25.\n",
    "2. TFIDF.\n",
    "3. N-GRAM - Diferences, Counts, ... (Word-Grams)\n",
    "4. Original, Cleaned and Stemmed statistics [Lengths, Counts and differeneces].\n",
    "\n",
    "Features we tried and didn't work good:\n",
    "5. Digits.\n",
    "6. Distances from train Mean lengths and RMS [Root mean square]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class BM25 :\n",
    "    def __init__(self, fn_docs, delimiter=' ') :\n",
    "        self.dictionary = corpora.Dictionary()\n",
    "        self.DF = {}\n",
    "        self.delimiter = delimiter\n",
    "        self.DocTF = []\n",
    "        self.DocIDF = {}\n",
    "        self.N = 0\n",
    "        self.DocAvgLen = 0\n",
    "        self.fn_docs = fn_docs\n",
    "        self.DocLen = []\n",
    "        self.buildDictionary()\n",
    "        self.TFIDF_Generator()\n",
    "\n",
    "    def buildDictionary(self) :\n",
    "        all_docs = []\n",
    "        for line in self.fn_docs:\n",
    "            all_docs.append(line.strip().split(self.delimiter))\n",
    "        self.dictionary.add_documents(all_docs)\n",
    "\n",
    "    def TFIDF_Generator(self, base=math.e) :\n",
    "        docTotalLen = 0\n",
    "        for line in self.fn_docs :\n",
    "            doc = line.strip().split(self.delimiter)\n",
    "            docTotalLen += len(doc)\n",
    "            self.DocLen.append(len(doc))\n",
    "            #print self.dictionary.doc2bow(doc)\n",
    "            bow = dict([(term, freq*1.0/len(doc)) for term, freq in self.dictionary.doc2bow(doc)])\n",
    "            for term, tf in bow.items() :\n",
    "                if term not in self.DF :\n",
    "                    self.DF[term] = 0\n",
    "                self.DF[term] += 1\n",
    "            self.DocTF.append(bow)\n",
    "            self.N = self.N + 1\n",
    "        for term in self.DF:\n",
    "            self.DocIDF[term] = math.log((self.N - self.DF[term] +0.5) / (self.DF[term] + 0.5), base)\n",
    "        self.DocAvgLen = docTotalLen / self.N\n",
    "\n",
    "    def BM25Score(self, Query=[], k1=1.5, b=0.75) :\n",
    "        query_bow = self.dictionary.doc2bow(Query)\n",
    "        scores = []\n",
    "        for idx, doc in enumerate(self.DocTF) :\n",
    "            commonTerms = set(dict(query_bow).keys()) & set(doc.keys())\n",
    "            tmp_score = []\n",
    "            doc_terms_len = self.DocLen[idx]\n",
    "            for term in commonTerms :\n",
    "                upper = (doc[term] * (k1+1))\n",
    "                below = ((doc[term]) + k1*(1 - b + b*doc_terms_len/self.DocAvgLen))\n",
    "                tmp_score.append(self.DocIDF[term] * upper / below)\n",
    "            scores.append(sum(tmp_score))\n",
    "        return scores\n",
    "\n",
    "    def TFIDF(self) :\n",
    "        tfidf = []\n",
    "        for doc in self.DocTF :\n",
    "            doc_tfidf  = [(term, tf*self.DocIDF[term]) for term, tf in doc.items()]\n",
    "            doc_tfidf.sort()\n",
    "            tfidf.append(doc_tfidf)\n",
    "        return tfidf\n",
    "\n",
    "    def Items(self) :\n",
    "        # Return a list [(term_idx, term_desc),]\n",
    "        items = self.dictionary.items()\n",
    "        items.sort()\n",
    "        return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureMapper:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            extractor.fit(X[column_name], y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted = []\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            #print column_name,feature_name\n",
    "            fea = extractor.transform(X[column_name])\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        extracted = []\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            fea = extractor.fit_transform(X[column_name], y)\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to sklearn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VECTOR_SIZE = 400\n",
    "class SimilarityTransform(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        new_arr = []\n",
    "        for row in X.toarray():\n",
    "            bow_query = row[:VECTOR_SIZE]\n",
    "            bow_title = row[VECTOR_SIZE:2*VECTOR_SIZE]\n",
    "            bow_desc = row[2*VECTOR_SIZE:3*VECTOR_SIZE]\n",
    "            cosine_query_title = cosine_similarity(bow_query,bow_title)[0]\n",
    "            cosine_query_desc = cosine_similarity(bow_query,bow_desc)[0]\n",
    "            cosine_title_desc = cosine_similarity(bow_title,bow_desc)[0]\n",
    "            new_arr.append(list(row)+list(cosine_query_title)+list(cosine_query_desc)+list(cosine_title_desc))\n",
    "        return new_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SimpleTransform(BaseEstimator):\n",
    "    def __init__(self, transformer=identity):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self.transformer(x) for x in X], ndmin=2).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# calc n_grams\n",
    "def getNGramsUnordered(n,string):\n",
    "    word_lst = token_pattern.findall(string)\n",
    "    grams = []\n",
    "    for i in xrange(len(word_lst)-n+1):\n",
    "        words = word_lst[i:i+n]\n",
    "        for gram in itertools.permutations(words):\n",
    "            grams.append('_'.join(list(gram)))\n",
    "    return grams\n",
    "\n",
    "def rms(lst):\n",
    "    return np.sqrt(np.mean(np.square(lst)))\n",
    "\n",
    "def mean_delta(lst):\n",
    "    return abs((lst - lst.shift(-1)).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_features(df):\n",
    "    # count init texts lenghts\n",
    "    df[\"query_init_len\"] = df[\"query\"].apply(lambda x: float(len(token_pattern.findall(x))) if x!='' else 0.0)\n",
    "    df[\"desc_init_len\"] = df[\"product_description\"].apply(lambda x: float(len(token_pattern.findall(x))) if x!='' else 0.0)\n",
    "    df[\"title_init_len\"] = df[\"product_title\"].apply(lambda x: float(len(token_pattern.findall(x))) if x!='' else 0.0)\n",
    "    print \"Done counting lenghts\"\n",
    "    \n",
    "    # count stemmed texts length\n",
    "    df[\"query_stemed_len\"] = df[\"query_stemed\"].apply(lambda x: float(len(token_pattern.findall(x))) if x!='' else 0.0)\n",
    "    df[\"product_description_stemed_len\"] = df[\"product_description_stemed\"].apply(lambda x: float(len(token_pattern.findall(x))) if x!='' else 0.0)\n",
    "    df[\"product_title_stemed_len\"] = df[\"product_title_stemed\"].apply(lambda x: float(len(token_pattern.findall(x))) if x!='' else 0.0)\n",
    "    print \"Done counting stemed lenghts\"\n",
    "    \n",
    "    # difference between init and stemed&cleaned texts\n",
    "    df[\"query_diff_len\"] = df[\"query_init_len\"]-df[\"query_stemed_len\"]\n",
    "    df[\"desc_diff_len\"] = df[\"desc_init_len\"]-df[\"product_description_stemed_len\"]\n",
    "    df[\"title_diff_len\"] = df[\"title_init_len\"]-df[\"product_title_stemed_len\"]\n",
    "    print \"Done calculate length differences\"\n",
    "    \n",
    "    # calc change ratio\n",
    "    df[\"query_change_ratio\"] = df[\"query_diff_len\"]/df[\"query_init_len\"]\n",
    "    df[\"query_change_ratio\"] = df[\"query_change_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    df[\"desc_change_ratio\"] = df[\"desc_diff_len\"]/df[\"desc_init_len\"]\n",
    "    df[\"desc_change_ratio\"] = df[\"desc_change_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    df[\"title_change_ratio\"] = df[\"title_diff_len\"]/df[\"title_init_len\"]\n",
    "    df[\"title_change_ratio\"] = df[\"title_change_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    print \"Done calc change ratio\"\n",
    "    \n",
    "    # calc length ratio\n",
    "    df[\"query_title_ratio\"] = df[\"query_stemed_len\"]/df[\"product_title_stemed_len\"]\n",
    "    df[\"query_title_ratio\"] = df[\"query_title_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    df[\"query_desc_ratio\"] = df[\"query_stemed_len\"]/df[\"product_description_stemed_len\"]\n",
    "    df[\"query_desc_ratio\"] = df[\"query_desc_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    df[\"title_desc_ratio\"] = df[\"product_title_stemed_len\"]/df[\"product_description_stemed_len\"]\n",
    "    df[\"title_desc_ratio\"] = df[\"title_desc_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    print \"Done calc length ratio\"\n",
    "    \n",
    "    # calc length ratio\n",
    "    df[\"query_title_ratio\"] = df[\"query_stemed_len\"]/df[\"product_title_stemed_len\"]\n",
    "    df[\"query_title_ratio\"] = df[\"query_title_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    df[\"query_desc_ratio\"] = df[\"query_stemed_len\"]/df[\"product_description_stemed_len\"]\n",
    "    df[\"query_desc_ratio\"] = df[\"query_desc_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    df[\"title_desc_ratio\"] = df[\"product_title_stemed_len\"]/df[\"product_description_stemed_len\"]\n",
    "    df[\"title_desc_ratio\"] = df[\"title_desc_ratio\"].replace([np.inf, -np.inf, np.nan],0)\n",
    "    print \"Done calc length ratio\"\n",
    "       \n",
    "    # empty description flag\n",
    "    df[\"no_desc\"] = df[\"product_description_stemed_len\"]==0\n",
    "    print \"Done flaging empty description\"\n",
    "    \n",
    "    # calc BM25\n",
    "    for i, row in df.iterrows():\n",
    "        bm25 = BM25([row[\"product_title_stemed\"],row[\"product_description_stemed\"]], delimiter=' ')\n",
    "        Query = row[\"query_stemed\"]\n",
    "        Query = Query.split(' ')\n",
    "        scores = bm25.BM25Score(Query)\n",
    "        df.set_value(i,\"BM25Title\",scores[0])\n",
    "        df.set_value(i,\"BM25Description\",scores[1])\n",
    "    print \"Done calc BM25\"\n",
    "    \n",
    "    # calc number of similar and unsimiliar grams\n",
    "    GRAMS = 3\n",
    "    for n in range(1,GRAMS+1):\n",
    "        #df[\"query_stemed_%sgram\"%n] = df[\"query_stemed\"].apply(lambda x: set(getNGramsUnordered(n,x)))\n",
    "        #df[\"desc_stemed_%sgram\"%n] = df[\"product_description_stemed\"].apply(lambda x: set(getNGramsUnordered(n,x)))\n",
    "        #df[\"title_stemed_%sgram\"%n] = df[\"product_title_stemed\"].apply(lambda x: set(getNGramsUnordered(n,x)))\n",
    "        for i, row in df.iterrows():\n",
    "            query_stemed_gram = set(getNGramsUnordered(n,row[\"query_stemed\"]))\n",
    "            title_stemed_gram = set(getNGramsUnordered(n,row[\"product_title_stemed\"]))\n",
    "            desc_stemed_gram = set(getNGramsUnordered(n,row[\"product_description_stemed\"]))\n",
    "            intersect1 = query_stemed_gram.intersection(title_stemed_gram)\n",
    "            intersect2 = query_stemed_gram.intersection(desc_stemed_gram)\n",
    "            intersect3 = title_stemed_gram.intersection(desc_stemed_gram)\n",
    "            df.set_value(i,\"query_title_similar_%sgram_len\"%n,float(len(intersect1)))\n",
    "            df.set_value(i,\"query_desc_similar_%sgram_len\"%n,float(len(intersect2)))\n",
    "            df.set_value(i,\"title_desc_similar_%sgram_len\"%n,float(len(intersect3)))\n",
    "            df.set_value(i,\"query_title_similar_%sgram_percent\"%n,float(len(intersect1))/row[\"product_title_stemed_len\"] if row[\"product_title_stemed_len\"]>0 else 0.0)\n",
    "            df.set_value(i,\"query_desc_similar_%sgram_percent\"%n,float(len(intersect2))/row[\"product_description_stemed_len\"] if row[\"product_description_stemed_len\"]>0 else 0.0)\n",
    "            df.set_value(i,\"title_desc_similar_%sgram_percent\"%n,float(len(intersect3))/row[\"product_description_stemed_len\"] if row[\"product_description_stemed_len\"]>0 else 0.0)\n",
    "            # Differences\n",
    "            difference_num1 = float(len(query_stemed_gram.difference(title_stemed_gram)))\n",
    "            difference_num2 = float(len(query_stemed_gram.difference(desc_stemed_gram)))\n",
    "            difference_num3 = float(len(title_stemed_gram.difference(desc_stemed_gram)))\n",
    "            difference_num4 = float(len(title_stemed_gram.difference(query_stemed_gram)))\n",
    "            difference_num5 = float(len(desc_stemed_gram.difference(query_stemed_gram)))\n",
    "            difference_num6 = float(len(desc_stemed_gram.difference(title_stemed_gram)))\n",
    "            sym_difference_num1 = float(len(query_stemed_gram.symmetric_difference(title_stemed_gram)))\n",
    "            sym_difference_num2 = float(len(query_stemed_gram.symmetric_difference(desc_stemed_gram)))\n",
    "            sym_difference_num3 = float(len(title_stemed_gram.symmetric_difference(desc_stemed_gram)))\n",
    "            \n",
    "            df.set_value(i,\"query_title_diff_%sgram\"%n, difference_num1)\n",
    "            df.set_value(i,\"query_desc_diff_%sgram\"%n, difference_num2)\n",
    "            df.set_value(i,\"title_desc_diff_%sgram\"%n, difference_num3)\n",
    "            df.set_value(i,\"title_query_diff_%sgram\"%n, difference_num4)\n",
    "            df.set_value(i,\"desc_query_diff_%sgram\"%n, difference_num5)\n",
    "            df.set_value(i,\"desc_title_diff_%sgram\"%n, difference_num6)\n",
    "            df.set_value(i,\"query_title_sym_diff_%sgram\"%n, sym_difference_num1)\n",
    "            df.set_value(i,\"query_desc_sym_diff_%sgram\"%n, sym_difference_num2)\n",
    "            df.set_value(i,\"title_desc_sym_diff_%sgram\"%n, sym_difference_num3)\n",
    "            \n",
    "            #\n",
    "            \n",
    "    print \"Done calc similar words\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # digits\n",
    "    for i, row in df.iterrows():\n",
    "        query, title, desc = row[\"query\"], row[\"product_title\"], row[\"product_description\"]\n",
    "\n",
    "        num_of_digits_query = len([float(w) for w in query if w.isdecimal()])\n",
    "        sum_of_digits_query = sum([float(w) for w in query if w.isdecimal()])\n",
    "        num_of_digits_title = len([float(w) for w in title if w.isdecimal()])\n",
    "        sum_of_digits_title = sum([float(w) for w in title if w.isdecimal()])\n",
    "        num_of_digits_desc = len([float(w) for w in desc if w.isdecimal()])\n",
    "        sum_of_digits_desc = sum([float(w) for w in desc if w.isdecimal()])\n",
    "        num_of_unique_digits_query = len(set([float(w) for w in query if w.isdecimal()]))\n",
    "        num_of_unique_digits_title = len(set([float(w) for w in title if w.isdecimal()]))\n",
    "        num_of_unique_digits_desc = len(set([float(w) for w in desc if w.isdecimal()]))\n",
    "        \n",
    "        df.set_value(i,\"num_of_digits_query\", num_of_digits_query)\n",
    "        df.set_value(i,\"num_of_digits_title\", num_of_digits_title)\n",
    "        df.set_value(i,\"num_of_digits_desc\", num_of_digits_desc)\n",
    "        \n",
    "        df.set_value(i,\"sum_of_digits_query\", sum_of_digits_query)\n",
    "        df.set_value(i,\"sum_of_digits_title\", sum_of_digits_title)\n",
    "        df.set_value(i,\"sum_of_digits_desc\", sum_of_digits_desc)\n",
    "        df.set_value(i,\"num_of_unique_digits_query\", num_of_unique_digits_query)\n",
    "        df.set_value(i,\"num_of_unique_digits_title\", num_of_unique_digits_title)\n",
    "        df.set_value(i,\"num_of_unique_digits_desc\", num_of_unique_digits_desc)\n",
    "    \n",
    "    \n",
    "    print 'Done calc digits...'\n",
    "    \n",
    "    \n",
    "    # Calc rms, avg\n",
    "    for i, row in df.iterrows():\n",
    "        local_train = train[train['query'] == row[1]]\n",
    "        desc_len, title_len = row['product_description_stemed_len'], row['product_title_stemed_len']\n",
    "\n",
    "        # distance from RMS\n",
    "        desc_local_rms, desc_global_rms = rms(train['product_description_stemed_len']), rms(local_train['product_description_stemed_len'])\n",
    "        title_local_rms, title_global_rms = rms(train['product_title_stemed_len']), rms(local_train['product_title_stemed_len'])\n",
    "\n",
    "        desc_dist_local_rms, desc_dist_rms = desc_len - desc_local_rms, desc_len - desc_global_rms\n",
    "        title_dist_local_rms, title_dist_rms = title_len - title_local_rms, title_len - title_global_rms\n",
    "\n",
    "        df.set_value(i,\"desc_dist_local_rms\", desc_dist_local_rms)\n",
    "        df.set_value(i,\"desc_dist_rms\", desc_dist_rms)\n",
    "        df.set_value(i,\"title_dist_local_rms\", title_dist_local_rms)\n",
    "        df.set_value(i,\"title_dist_rms\", title_dist_rms)\n",
    "        \n",
    "        # distance from Average\n",
    "        desc_global_len_avg, desc_local_len_avg = train['product_description_stemed_len'].mean(), local_train['product_description_stemed_len'].mean()\n",
    "        title_global_len_avg, title_local_len_avg = train['product_description_stemed_len'].mean(), local_train['product_description_stemed_len'].mean()\n",
    "\n",
    "        desc_dist_local_avg, desc_dist_avg = desc_len - desc_local_len_avg, desc_len - desc_global_len_avg\n",
    "        title_dist_local_avg, title_dist_avg = title_len - title_local_len_avg, title_len - title_global_len_avg\n",
    "        \n",
    "        df.set_value(i,\"desc_dist_local_avg\", desc_dist_local_avg)\n",
    "        df.set_value(i,\"desc_dist_avg\", desc_dist_avg)\n",
    "        df.set_value(i,\"title_dist_local_avg\", title_dist_local_avg)\n",
    "        df.set_value(i,\"title_dist_avg\", title_dist_avg)\n",
    "    \n",
    "    print \"Done average, RMS\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # calc local tfidf\n",
    "    for i, row in df.iterrows():\n",
    "        res = TfidfVectorizer(max_features=VECTOR_SIZE).fit_transform([row[\"query_stemed\"],row[\"product_title_stemed\"],row[\"product_description_stemed\"]])\n",
    "        # print res.getnnz(),res\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done counting lenghts\n",
      "Done counting stemed lenghts\n",
      "Done calculate length differences\n",
      "Done calc change ratio\n",
      "Done calc length ratio\n",
      "Done calc length ratio\n",
      "Done flaging empty description\n",
      "Done calc BM25\n",
      "Done calc similar words\n"
     ]
    }
   ],
   "source": [
    "train = extract_features(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#                          Feature Set Name            Data Frame Column              Transformer\n",
    "features = FeatureUnion(transformer_list=[('QueryBagOfWords',Pipeline([('selector', ItemSelector(key='query_stemed')),\n",
    "                                                                      ('vectorizer',CountVectorizer(max_features=VECTOR_SIZE))])),\n",
    "                                          ('TitleBagOfWords',Pipeline([('selector', ItemSelector(key='product_title_stemed')),\n",
    "                                                                      ('vectorizer',CountVectorizer(max_features=VECTOR_SIZE))])),\n",
    "                                          ('DescriptionBagOfWords',Pipeline([('selector', ItemSelector(key='product_description_stemed')),\n",
    "                                                                      ('vectorizer',CountVectorizer(max_features=VECTOR_SIZE))])),\n",
    "                                          ('QueryTFIDF',Pipeline([('selector', ItemSelector(key='query_stemed')),\n",
    "                                                                      ('vectorizer',TfidfVectorizer(max_features=VECTOR_SIZE))])),\n",
    "                                          ('TitleTFIDF',Pipeline([('selector', ItemSelector(key='product_title_stemed')),\n",
    "                                                                      ('vectorizer',TfidfVectorizer(max_features=VECTOR_SIZE))])),\n",
    "                                          ('DescriptionTFIDF',Pipeline([('selector', ItemSelector(key='product_description_stemed')),\n",
    "                                                                      ('vectorizer',TfidfVectorizer(max_features=VECTOR_SIZE))])),\n",
    "                                          ('QueryTokensInTitle1',Pipeline([('selector', ItemSelector(key='query_title_similar_1gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInDescription1',Pipeline([('selector', ItemSelector(key='query_desc_similar_1gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleTokensInDescription1',Pipeline([('selector', ItemSelector(key='title_desc_similar_1gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInTitle2',Pipeline([('selector', ItemSelector(key='query_title_similar_2gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInDescription2',Pipeline([('selector', ItemSelector(key='query_desc_similar_2gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleTokensInDescription2',Pipeline([('selector', ItemSelector(key='title_desc_similar_2gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInTitle3',Pipeline([('selector', ItemSelector(key='query_title_similar_3gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInDescription3',Pipeline([('selector', ItemSelector(key='query_desc_similar_3gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleTokensInDescription3',Pipeline([('selector', ItemSelector(key='title_desc_similar_3gram_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInTitle1Percent',Pipeline([('selector', ItemSelector(key='query_title_similar_1gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInDescription1Percent',Pipeline([('selector', ItemSelector(key='query_desc_similar_1gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleTokensInDescription1Percent',Pipeline([('selector', ItemSelector(key='title_desc_similar_1gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInTitle2Percent',Pipeline([('selector', ItemSelector(key='query_title_similar_2gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInDescription2Percent',Pipeline([('selector', ItemSelector(key='query_desc_similar_2gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleTokensInDescription2Percent',Pipeline([('selector', ItemSelector(key='title_desc_similar_2gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInTitle3Percent',Pipeline([('selector', ItemSelector(key='query_title_similar_3gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTokensInDescription3Percent',Pipeline([('selector', ItemSelector(key='query_desc_similar_3gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleTokensInDescription3Percent',Pipeline([('selector', ItemSelector(key='title_desc_similar_3gram_percent')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryBM25WithTitle',Pipeline([('selector', ItemSelector(key='BM25Title')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryBM25WithDesc',Pipeline([('selector', ItemSelector(key='BM25Description')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('DescriptionLength',Pipeline([('selector', ItemSelector(key='product_description_stemed_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleLength',Pipeline([('selector', ItemSelector(key='product_title_stemed_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryLength',Pipeline([('selector', ItemSelector(key='query_stemed_len')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryTitleRatio',Pipeline([('selector', ItemSelector(key='query_title_ratio')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryDescRatio',Pipeline([('selector', ItemSelector(key='query_desc_ratio')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleDescRatio',Pipeline([('selector', ItemSelector(key='title_desc_ratio')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('DescriptionCleanRatio',Pipeline([('selector', ItemSelector(key='desc_change_ratio')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('TitleCleanRatio',Pipeline([('selector', ItemSelector(key='title_change_ratio')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('QueryCleanRatio',Pipeline([('selector', ItemSelector(key='query_change_ratio')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('NoDescFlag',Pipeline([('selector', ItemSelector(key='no_desc')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "\n",
    "                                          \n",
    "                                          ####### 1 grram\n",
    "                                          ('diff_1',Pipeline([('selector', ItemSelector(key='query_title_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_2',Pipeline([('selector', ItemSelector(key='query_desc_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_3',Pipeline([('selector', ItemSelector(key='title_desc_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_4',Pipeline([('selector', ItemSelector(key='title_query_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_5',Pipeline([('selector', ItemSelector(key='desc_query_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_6',Pipeline([('selector', ItemSelector(key='desc_title_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_1',Pipeline([('selector', ItemSelector(key='query_title_sym_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_2',Pipeline([('selector', ItemSelector(key='query_desc_sym_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_3',Pipeline([('selector', ItemSelector(key='title_desc_sym_diff_1gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          # 2 gram\n",
    "                                          ('diff_1_2',Pipeline([('selector', ItemSelector(key='query_title_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_2_2',Pipeline([('selector', ItemSelector(key='query_desc_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_3_2',Pipeline([('selector', ItemSelector(key='title_desc_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_4_2',Pipeline([('selector', ItemSelector(key='title_query_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_5_2',Pipeline([('selector', ItemSelector(key='desc_query_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_6_2',Pipeline([('selector', ItemSelector(key='desc_title_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_1_2',Pipeline([('selector', ItemSelector(key='query_title_sym_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_2_2',Pipeline([('selector', ItemSelector(key='query_desc_sym_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_3_2',Pipeline([('selector', ItemSelector(key='title_desc_sym_diff_2gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          # 3_gram\n",
    "                                          ('diff_1_3',Pipeline([('selector', ItemSelector(key='query_title_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_2_3',Pipeline([('selector', ItemSelector(key='query_desc_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_3_3',Pipeline([('selector', ItemSelector(key='title_desc_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_4_3',Pipeline([('selector', ItemSelector(key='title_query_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_5_3',Pipeline([('selector', ItemSelector(key='desc_query_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('diff_6_3',Pipeline([('selector', ItemSelector(key='desc_title_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_1_3',Pipeline([('selector', ItemSelector(key='query_title_sym_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_2_3',Pipeline([('selector', ItemSelector(key='query_desc_sym_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())])),\n",
    "                                          ('sym_diff_3_3',Pipeline([('selector', ItemSelector(key='title_desc_sym_diff_3gram')),\n",
    "                                                                      ('simple',SimpleTransform())]))\n",
    "                                          \n",
    "                                          \n",
    "\n",
    "                                         ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RMS, Average...\n",
    "#('desc_dist_local_rms',Pipeline([('selector', ItemSelector(key='desc_dist_local_rms')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('desc_dist_rms',Pipeline([('selector', ItemSelector(key='desc_dist_rms')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('title_dist_local_rms',Pipeline([('selector', ItemSelector(key='title_dist_local_rms')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('title_dist_rms',Pipeline([('selector', ItemSelector(key='title_dist_rms')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('desc_dist_local_avg',Pipeline([('selector', ItemSelector(key='desc_dist_local_avg')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('desc_dist_avg',Pipeline([('selector', ItemSelector(key='desc_dist_avg')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('title_dist_local_avg',Pipeline([('selector', ItemSelector(key='title_dist_local_avg')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('title_dist_avg',Pipeline([('selector', ItemSelector(key='title_dist_avg')),\n",
    "#                         ('simple',SimpleTransform())])),\n",
    "\n",
    " # Digits\n",
    "#('num_of_digits_query',Pipeline([('selector', ItemSelector(key='num_of_digits_query')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('num_of_digits_title',Pipeline([('selector', ItemSelector(key='num_of_digits_title')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('num_of_digits_desc',Pipeline([('selector', ItemSelector(key='num_of_digits_desc')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('sum_of_digits_query',Pipeline([('selector', ItemSelector(key='sum_of_digits_query')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('sum_of_digits_title',Pipeline([('selector', ItemSelector(key='sum_of_digits_title')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('sum_of_digits_desc',Pipeline([('selector', ItemSelector(key='sum_of_digits_desc')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('num_of_unique_digits_query',Pipeline([('selector', ItemSelector(key='num_of_unique_digits_query')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('num_of_unique_digits_title',Pipeline([('selector', ItemSelector(key='num_of_unique_digits_title')),\n",
    "#                          ('simple',SimpleTransform())])),\n",
    "#('num_of_unique_digits_desc',Pipeline([('selector', ItemSelector(key='num_of_unique_digits_desc')),\n",
    "#                          ('simple',SimpleTransform())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "At start We've tested all the classifiers below. [And it took about 8 hours to evaluate them all]\n",
    "In ALL of our evaluations 'ExtraTreesRegressor' gave the best results, So we commented out all the other classifiers [To save time]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    #\"RandomForestRegressor\":Pipeline([(\"extract_features\", features),\n",
    "    #                                  ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", RandomForestRegressor(n_estimators=100,n_jobs=-1))]),\n",
    "    #\"RandomForestClassifier\":Pipeline([(\"extract_features\", features),\n",
    "    #                                  ('createMore',SimilarityTransform()),\n",
    "    #                 (\"classify\", RandomForestClassifier(n_estimators=100,n_jobs=-1))]),\n",
    "    #\"LogisticRegression\":Pipeline([(\"extract_features\", features),\n",
    "    #                ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", LogisticRegression())]),\n",
    "    #\"SGDRegressor\":Pipeline([(\"extract_features\", features),\n",
    "    #                ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", SGDRegressor())]),\n",
    "    #\"PassiveAggressiveRegressor\":Pipeline([(\"extract_features\", features),\n",
    "    #                ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", PassiveAggressiveRegressor())]),\n",
    "    #\"LassoLars\":Pipeline([(\"extract_features\", features),\n",
    "    #                ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", LassoLars())]),\n",
    "    #'SVR':Pipeline([(\"extract_features\", features),\n",
    "    #                ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", SVR())]),\n",
    "    #'SVC':Pipeline([(\"extract_features\", features),\n",
    "    #                ('createMore',SimilarityTransform()),\n",
    "    #                 (\"classify\", SVC())]),\n",
    "    #'BaggingRegressor':Pipeline([(\"extract_features\", features),\n",
    "    #                            ('createMore',SimilarityTransform()),\n",
    "    #                 (\"regress\", BaggingRegressor(n_jobs=-1))]),\n",
    "    'ExtraTreesRegressor':Pipeline([(\"extract_features\", features),\n",
    "                                    ('createMore',SimilarityTransform()),\n",
    "                     (\"regress\", ExtraTreesRegressor(n_estimators=100,n_jobs=-1))]),\n",
    "    #'GradientBoostingRegressor':Pipeline([(\"extract_features\", features),\n",
    "    #                                      ('createMore',SimilarityTransform()),\n",
    "    #                                     (\"regress\", GradientBoostingRegressor())])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predTrans(pred):\n",
    "    return int(round(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following 3 functions have been taken from Ben Hamner's github repository\n",
    "# https://github.com/benhamner/Metrics\n",
    "def confusion_matrix(rater_a, rater_b, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the confusion matrix between rater's ratings\n",
    "    \"\"\"\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(rater_a + rater_b)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(rater_a + rater_b)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    conf_mat = [[0 for i in range(num_ratings)]\n",
    "                for j in range(num_ratings)]\n",
    "    for a, b in zip(rater_a, rater_b):\n",
    "        conf_mat[a - min_rating][b - min_rating] += 1\n",
    "    return conf_mat\n",
    "\n",
    "\n",
    "def histogram(ratings, min_rating=None, max_rating=None):\n",
    "    \"\"\"\n",
    "    Returns the counts of each type of rating that a rater made\n",
    "    \"\"\"\n",
    "    if min_rating is None:\n",
    "        min_rating = min(ratings)\n",
    "    if max_rating is None:\n",
    "        max_rating = max(ratings)\n",
    "    num_ratings = int(max_rating - min_rating + 1)\n",
    "    hist_ratings = [0 for x in range(num_ratings)]\n",
    "    for r in ratings:\n",
    "        hist_ratings[r - min_rating] += 1\n",
    "    return hist_ratings\n",
    "\n",
    "\n",
    "def quadratic_weighted_kappa(y, y_pred):\n",
    "    \"\"\"\n",
    "    Calculates the quadratic weighted kappa\n",
    "    axquadratic_weighted_kappa calculates the quadratic weighted kappa\n",
    "    value, which is a measure of inter-rater agreement between two raters\n",
    "    that provide discrete numeric ratings.  Potential values range from -1\n",
    "    (representing complete disagreement) to 1 (representing complete\n",
    "    agreement).  A kappa value of 0 is expected if all agreement is due to\n",
    "    chance.\n",
    "    quadratic_weighted_kappa(rater_a, rater_b), where rater_a and rater_b\n",
    "    each correspond to a list of integer ratings.  These lists must have the\n",
    "    same length.\n",
    "    The ratings should be integers, and it is assumed that they contain\n",
    "    the complete range of possible ratings.\n",
    "    quadratic_weighted_kappa(X, min_rating, max_rating), where min_rating\n",
    "    is the minimum possible rating, and max_rating is the maximum possible\n",
    "    rating\n",
    "    \"\"\"\n",
    "    rater_a = y\n",
    "    rater_b = y_pred\n",
    "    min_rating=None\n",
    "    max_rating=None\n",
    "    rater_a = np.array(rater_a, dtype=int)\n",
    "    rater_b = np.array(rater_b, dtype=int)\n",
    "    assert(len(rater_a) == len(rater_b))\n",
    "    if min_rating is None:\n",
    "        min_rating = min(min(rater_a), min(rater_b))\n",
    "    if max_rating is None:\n",
    "        max_rating = max(max(rater_a), max(rater_b))\n",
    "    conf_mat = confusion_matrix(rater_a, rater_b,\n",
    "                                min_rating, max_rating)\n",
    "    num_ratings = len(conf_mat)\n",
    "    num_scored_items = float(len(rater_a))\n",
    "\n",
    "    hist_rater_a = histogram(rater_a, min_rating, max_rating)\n",
    "    hist_rater_b = histogram(rater_b, min_rating, max_rating)\n",
    "\n",
    "    numerator = 0.0\n",
    "    denominator = 0.0\n",
    "\n",
    "    for i in range(num_ratings):\n",
    "        for j in range(num_ratings):\n",
    "            expected_count = (hist_rater_a[i] * hist_rater_b[j]\n",
    "                              / num_scored_items)\n",
    "            d = pow(i - j, 2.0) / pow(num_ratings - 1, 2.0)\n",
    "            numerator += d * conf_mat[i][j] / num_scored_items\n",
    "            denominator += d * expected_count / num_scored_items\n",
    "\n",
    "    return (1.0 - numerator / denominator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-Process:\n",
    "It is worth to mention that the classifier gave non-integer results and we had to find an optimal way to decide where to map it. So we tried 4 different approachs:\n",
    "1. Floor.\n",
    "2. Ceil.\n",
    "3. Round.\n",
    "4. CFD - Enforcing the train data distribution of median revelance on the predicted results.\n",
    "The rank from worst to best in each of the times was: Floor (As worst), Ceil, Round, CFD (Best)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def probabilities_median_table():\n",
    "    coun = Counter(train['median_relevance'])\n",
    "    max_feat = len(train['median_relevance'])\n",
    "    return [float(median)/max_feat for median in coun.values()]\n",
    "\n",
    "def prediction_distributions(pred):\n",
    "    df = pd.DataFrame(data=pred, columns=['Prediction'])\n",
    "    df.sort(columns=['Prediction'], ascending=True, inplace=True)\n",
    "    probs = probabilities_median_table()\n",
    "    \n",
    "    num_of_one_median, num_of_two_median = int(math.floor(probs[0]*len(df))), int(math.floor(probs[1]*len(df)))\n",
    "    num_of_three_median = int(math.floor(probs[2]*len(df)))\n",
    "    \n",
    "    for i, row in df.iterrows():\n",
    "        df.set_value(i,\"floor_prediction\", math.floor(row['Prediction']))\n",
    "        df.set_value(i,\"ceil_prediction\", math.ceil(row['Prediction']))\n",
    "        df.set_value(i,\"round_prediction\", round(row['Prediction']))\n",
    "\n",
    "        if num_of_one_median > 0:\n",
    "            num_of_one_median -= 1\n",
    "            df.set_value(i,\"cfd_prediction\", 1)\n",
    "            continue\n",
    "        if num_of_two_median > 0:\n",
    "            num_of_two_median -= 1\n",
    "            df.set_value(i,\"cfd_prediction\", 2)\n",
    "            continue\n",
    "        if num_of_three_median > 0:\n",
    "            num_of_three_median -= 1\n",
    "            df.set_value(i,\"cfd_prediction\", 3)\n",
    "            continue\n",
    "\n",
    "        df.set_value(i,\"cfd_prediction\", 4)\n",
    "        \n",
    "    df.sort_index(ascending=True, inplace=True)\n",
    "    return df['cfd_prediction'].tolist(), df['round_prediction'].tolist(), df['ceil_prediction'].tolist(), df['floor_prediction'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUMBER_OF_FOLDS = 3\n",
    "kfold = StratifiedKFold(list(train[\"median_relevance\"]),n_folds=NUMBER_OF_FOLDS)\n",
    "def evaluateClf(clf):\n",
    "    acc_score = []\n",
    "    for train_index, test_index in kfold:\n",
    "        X_train, X_test = train.iloc[train_index], train.iloc[test_index]\n",
    "        y_train, y_test = train[\"median_relevance\"].iloc[train_index], train[\"median_relevance\"].iloc[test_index]\n",
    "        clf.fit(X_train,y_train)\n",
    "        preds = [pred for pred in clf.predict(X_test)]\n",
    "        cfd_dest, round_dest, ceil_dest, floor_dest = prediction_distributions(preds)\n",
    "        score_cfd = quadratic_weighted_kappa(y_test, cfd_dest)\n",
    "        print \"Score is: %s.\"%(score_cfd)\n",
    "        acc_score.append(score_cfd)\n",
    "    return clf,np.mean(acc_score),np.std(acc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fitted_clfs = {}\n",
    "def chooseBest():\n",
    "    best_clf = None,0.0\n",
    "    for name,clf in clfs.items():\n",
    "        print \"Evaluating %s\"%name\n",
    "        fitted_clf,avg_score,std = evaluateClf(clf)\n",
    "        #fitted_clfs[name] = fitted_clf\n",
    "        print \"%s scored: %s with std: %s\"%(name,avg_score,std)\n",
    "        if best_clf[1]<avg_score:\n",
    "            print \"%s is currently the best\"%(name)\n",
    "            best_clf = fitted_clf,avg_score,std\n",
    "    return best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating ExtraTreesRegressor\n",
      "Score is: 0.622780015218.\n",
      "Score is: 0.620322556033.\n",
      "Score is: 0.627694933586.\n",
      "ExtraTreesRegressor scored: 0.623599168279 with std: 0.00306499010315\n",
      "ExtraTreesRegressor is currently the best\n"
     ]
    }
   ],
   "source": [
    "best_clf = chooseBest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('extract_features', FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('QueryBagOfWords', Pipeline(steps=[('selector', ItemSelector(key='query_stemed')), ('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.int64'>, encoding=u'utf-8'...imators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False))]),\n",
       " 0.6235991682790113,\n",
       " 0.0030649901031484347)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training\n"
     ]
    }
   ],
   "source": [
    "best_clf[0].fit(train,train[\"median_relevance\"])\n",
    "print \"Done training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"clf.pkl\",'wb') as f:\n",
    "    pickle.dump(best_clf,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done cleaning HTML\n",
      "Done removing stopwords\n",
      "Done stemming\n",
      "Done text Preprocess\n"
     ]
    }
   ],
   "source": [
    "test = preprocess(test)\n",
    "print \"Done text Preprocess\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done counting lenghts\n",
      "Done counting stemed lenghts\n",
      "Done calculate length differences\n",
      "Done calc change ratio\n",
      "Done calc length ratio\n",
      "Done calc length ratio\n",
      "Done flaging empty description\n",
      "Done calc BM25\n",
      "Done calc similar words\n",
      "Done text Feature Extraction\n"
     ]
    }
   ],
   "source": [
    "test = extract_features(test)\n",
    "print \"Done text Feature Extraction\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'query',\n",
       " 'product_title',\n",
       " 'product_description',\n",
       " 'product_description_clean',\n",
       " 'product_title_clean',\n",
       " 'query_clean',\n",
       " 'product_description_stemed',\n",
       " 'product_title_stemed',\n",
       " 'query_stemed',\n",
       " 'query_init_len',\n",
       " 'desc_init_len',\n",
       " 'title_init_len',\n",
       " 'query_stemed_len',\n",
       " 'product_description_stemed_len',\n",
       " 'product_title_stemed_len',\n",
       " 'query_diff_len',\n",
       " 'desc_diff_len',\n",
       " 'title_diff_len',\n",
       " 'query_change_ratio',\n",
       " 'desc_change_ratio',\n",
       " 'title_change_ratio',\n",
       " 'query_title_ratio',\n",
       " 'query_desc_ratio',\n",
       " 'title_desc_ratio',\n",
       " 'no_desc',\n",
       " 'BM25Title',\n",
       " 'BM25Description',\n",
       " 'query_title_similar_1gram_len',\n",
       " 'query_desc_similar_1gram_len',\n",
       " 'title_desc_similar_1gram_len',\n",
       " 'query_title_similar_1gram_percent',\n",
       " 'query_desc_similar_1gram_percent',\n",
       " 'title_desc_similar_1gram_percent',\n",
       " 'query_title_diff_1gram',\n",
       " 'query_desc_diff_1gram',\n",
       " 'title_desc_diff_1gram',\n",
       " 'title_query_diff_1gram',\n",
       " 'desc_query_diff_1gram',\n",
       " 'desc_title_diff_1gram',\n",
       " 'query_title_sym_diff_1gram',\n",
       " 'query_desc_sym_diff_1gram',\n",
       " 'title_desc_sym_diff_1gram',\n",
       " 'query_title_similar_2gram_len',\n",
       " 'query_desc_similar_2gram_len',\n",
       " 'title_desc_similar_2gram_len',\n",
       " 'query_title_similar_2gram_percent',\n",
       " 'query_desc_similar_2gram_percent',\n",
       " 'title_desc_similar_2gram_percent',\n",
       " 'query_title_diff_2gram',\n",
       " 'query_desc_diff_2gram',\n",
       " 'title_desc_diff_2gram',\n",
       " 'title_query_diff_2gram',\n",
       " 'desc_query_diff_2gram',\n",
       " 'desc_title_diff_2gram',\n",
       " 'query_title_sym_diff_2gram',\n",
       " 'query_desc_sym_diff_2gram',\n",
       " 'title_desc_sym_diff_2gram',\n",
       " 'query_title_similar_3gram_len',\n",
       " 'query_desc_similar_3gram_len',\n",
       " 'title_desc_similar_3gram_len',\n",
       " 'query_title_similar_3gram_percent',\n",
       " 'query_desc_similar_3gram_percent',\n",
       " 'title_desc_similar_3gram_percent',\n",
       " 'query_title_diff_3gram',\n",
       " 'query_desc_diff_3gram',\n",
       " 'title_desc_diff_3gram',\n",
       " 'title_query_diff_3gram',\n",
       " 'desc_query_diff_3gram',\n",
       " 'desc_title_diff_3gram',\n",
       " 'query_title_sym_diff_3gram',\n",
       " 'query_desc_sym_diff_3gram',\n",
       " 'title_desc_sym_diff_3gram']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"clf.pkl\",'rb') as f:\n",
    "    best_clf = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Pipeline(steps=[('extract_features', FeatureUnion(n_jobs=1,\n",
       "        transformer_list=[('QueryBagOfWords', Pipeline(steps=[('selector', ItemSelector(key='query_stemed')), ('vectorizer', CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "         dtype=<type 'numpy.int64'>, encoding=u'utf-8'...imators=100, n_jobs=-1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False))]),\n",
       " 0.6235991682790113,\n",
       " 0.0030649901031484347)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22513, 73)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22513, 2)\n"
     ]
    }
   ],
   "source": [
    "total_submission = pd.DataFrame()\n",
    "BATCH = 500\n",
    "i=0\n",
    "while i*BATCH<test.shape[0]:\n",
    "    predictions = [pred for pred in best_clf[0].predict(test[:][BATCH*i:BATCH*(i+1)])]\n",
    "    cfd_dest, round_dest, ceil_dest, floor_dest = prediction_distributions(predictions)\n",
    "    cfd_dest = [int(num) for num in cfd_dest]\n",
    "    submission = pd.DataFrame({\"id\": test[:][BATCH*i:BATCH*(i+1)][\"id\"], \"prediction\": cfd_dest})\n",
    "    total_submission = pd.concat([total_submission,submission])\n",
    "    i += 1\n",
    "print total_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total_submission.to_csv(\"res.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>query_init_len</th>\n",
       "      <th>desc_init_len</th>\n",
       "      <th>title_init_len</th>\n",
       "      <th>query_stemed_len</th>\n",
       "      <th>product_description_stemed_len</th>\n",
       "      <th>product_title_stemed_len</th>\n",
       "      <th>query_diff_len</th>\n",
       "      <th>desc_diff_len</th>\n",
       "      <th>title_diff_len</th>\n",
       "      <th>...</th>\n",
       "      <th>title_desc_similar_3gram_percent</th>\n",
       "      <th>query_title_diff_3gram</th>\n",
       "      <th>query_desc_diff_3gram</th>\n",
       "      <th>title_desc_diff_3gram</th>\n",
       "      <th>title_query_diff_3gram</th>\n",
       "      <th>desc_query_diff_3gram</th>\n",
       "      <th>desc_title_diff_3gram</th>\n",
       "      <th>query_title_sym_diff_3gram</th>\n",
       "      <th>query_desc_sym_diff_3gram</th>\n",
       "      <th>title_desc_sym_diff_3gram</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "      <td>22513.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>16328.282992</td>\n",
       "      <td>2.356505</td>\n",
       "      <td>65.717363</td>\n",
       "      <td>8.443477</td>\n",
       "      <td>2.296362</td>\n",
       "      <td>49.224359</td>\n",
       "      <td>8.150402</td>\n",
       "      <td>0.060143</td>\n",
       "      <td>16.493004</td>\n",
       "      <td>0.293075</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175269</td>\n",
       "      <td>1.955937</td>\n",
       "      <td>2.161418</td>\n",
       "      <td>29.710212</td>\n",
       "      <td>36.020433</td>\n",
       "      <td>265.157065</td>\n",
       "      <td>258.641363</td>\n",
       "      <td>37.976369</td>\n",
       "      <td>267.318483</td>\n",
       "      <td>288.351575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9424.576451</td>\n",
       "      <td>0.845015</td>\n",
       "      <td>118.010674</td>\n",
       "      <td>3.267144</td>\n",
       "      <td>0.775212</td>\n",
       "      <td>99.341340</td>\n",
       "      <td>3.040978</td>\n",
       "      <td>0.261427</td>\n",
       "      <td>26.046574</td>\n",
       "      <td>0.610858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.446344</td>\n",
       "      <td>3.569497</td>\n",
       "      <td>3.740122</td>\n",
       "      <td>19.826356</td>\n",
       "      <td>17.896238</td>\n",
       "      <td>455.232832</td>\n",
       "      <td>452.748355</td>\n",
       "      <td>18.809260</td>\n",
       "      <td>455.214390</td>\n",
       "      <td>452.857089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8201.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>16329.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>156.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>162.000000</td>\n",
       "      <td>186.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>24464.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>85.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>336.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>348.000000</td>\n",
       "      <td>366.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>32671.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2984.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>2658.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1097.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.400000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>11671.000000</td>\n",
       "      <td>11671.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>11677.000000</td>\n",
       "      <td>11725.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id  query_init_len  desc_init_len  title_init_len  \\\n",
       "count  22513.000000    22513.000000   22513.000000    22513.000000   \n",
       "mean   16328.282992        2.356505      65.717363        8.443477   \n",
       "std     9424.576451        0.845015     118.010674        3.267144   \n",
       "min        3.000000        1.000000       0.000000        0.000000   \n",
       "25%     8201.000000        2.000000       6.000000        6.000000   \n",
       "50%    16329.000000        2.000000      40.000000        8.000000   \n",
       "75%    24464.000000        3.000000      85.000000       10.000000   \n",
       "max    32671.000000        6.000000    2984.000000       42.000000   \n",
       "\n",
       "       query_stemed_len  product_description_stemed_len  \\\n",
       "count      22513.000000                    22513.000000   \n",
       "mean           2.296362                       49.224359   \n",
       "std            0.775212                       99.341340   \n",
       "min            1.000000                        0.000000   \n",
       "25%            2.000000                        6.000000   \n",
       "50%            2.000000                       29.000000   \n",
       "75%            3.000000                       62.000000   \n",
       "max            6.000000                     2658.000000   \n",
       "\n",
       "       product_title_stemed_len  query_diff_len  desc_diff_len  \\\n",
       "count              22513.000000    22513.000000   22513.000000   \n",
       "mean                   8.150402        0.060143      16.493004   \n",
       "std                    3.040978        0.261427      26.046574   \n",
       "min                    0.000000        0.000000       0.000000   \n",
       "25%                    6.000000        0.000000       0.000000   \n",
       "50%                    8.000000        0.000000      10.000000   \n",
       "75%                   10.000000        0.000000      22.000000   \n",
       "max                   38.000000        2.000000    1097.000000   \n",
       "\n",
       "       title_diff_len            ...              \\\n",
       "count    22513.000000            ...               \n",
       "mean         0.293075            ...               \n",
       "std          0.610858            ...               \n",
       "min          0.000000            ...               \n",
       "25%          0.000000            ...               \n",
       "50%          0.000000            ...               \n",
       "75%          0.000000            ...               \n",
       "max         10.000000            ...               \n",
       "\n",
       "       title_desc_similar_3gram_percent  query_title_diff_3gram  \\\n",
       "count                      22513.000000            22513.000000   \n",
       "mean                           0.175269                1.955937   \n",
       "std                            0.446344                3.569497   \n",
       "min                            0.000000                0.000000   \n",
       "25%                            0.000000                0.000000   \n",
       "50%                            0.000000                0.000000   \n",
       "75%                            0.181818                6.000000   \n",
       "max                            5.400000               24.000000   \n",
       "\n",
       "       query_desc_diff_3gram  title_desc_diff_3gram  title_query_diff_3gram  \\\n",
       "count           22513.000000           22513.000000            22513.000000   \n",
       "mean                2.161418              29.710212               36.020433   \n",
       "std                 3.740122              19.826356               17.896238   \n",
       "min                 0.000000               0.000000                0.000000   \n",
       "25%                 0.000000              18.000000               24.000000   \n",
       "50%                 0.000000              30.000000               36.000000   \n",
       "75%                 6.000000              42.000000               48.000000   \n",
       "max                24.000000             186.000000              204.000000   \n",
       "\n",
       "       desc_query_diff_3gram desc_title_diff_3gram  \\\n",
       "count           22513.000000          22513.000000   \n",
       "mean              265.157065            258.641363   \n",
       "std               455.232832            452.748355   \n",
       "min                 0.000000              0.000000   \n",
       "25%                24.000000             12.000000   \n",
       "50%               162.000000            156.000000   \n",
       "75%               348.000000            336.000000   \n",
       "max             11671.000000          11671.000000   \n",
       "\n",
       "       query_title_sym_diff_3gram  query_desc_sym_diff_3gram  \\\n",
       "count                22513.000000               22513.000000   \n",
       "mean                    37.976369                 267.318483   \n",
       "std                     18.809260                 455.214390   \n",
       "min                      0.000000                   0.000000   \n",
       "25%                     24.000000                  24.000000   \n",
       "50%                     36.000000                 162.000000   \n",
       "75%                     48.000000                 348.000000   \n",
       "max                    204.000000               11677.000000   \n",
       "\n",
       "       title_desc_sym_diff_3gram  \n",
       "count               22513.000000  \n",
       "mean                  288.351575  \n",
       "std                   452.857089  \n",
       "min                     0.000000  \n",
       "25%                    63.000000  \n",
       "50%                   186.000000  \n",
       "75%                   366.000000  \n",
       "max                 11725.000000  \n",
       "\n",
       "[8 rows x 64 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
